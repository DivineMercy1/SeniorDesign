# RFS - Senior Design 2019 Predictive Mainentance Read me
## Overview of code
* Environments used: VS Code, MYSQL Workbench, Anaconda Navigator
## Anaconda Libraries downloaded
* Note, one or more of the following environments are probably not required and was given/installed when Python was installed. The <span style="color:red">***italized***</span> items are the ones that have been explicitly installed in order to run the program.
* All of these environments are included to ensure comprehensiveness of the project.
  1. Alabaster
  2. Anaconda
  3. Anaconda-client
  4. Anaconda-project
  5. asn1crypto
  6. astroid
  7. astropy
  8. atomicwrites
  9. attrs
  10. babel
  11. backcall
  12. backports
  13. backports.os
  14. backports.shutil_g
  15. beautifulsoup4
  16. bitarray
  17. bkcharts
  18. blas
  19. blaze
  20. bleach
  21. blosc
  22. bokeh
  23. boto
  24. bottleneck
  25. bzip2
  26. ca-certificates
  27. certifi
  28. cffi
  29. chardet
  30. click
  31. cloudpickle
  32. clyent
  33. colorama
  34. comtypes
  35. conda
  36. conda-build
  37. conda-env
  38. conda-verify
  39. console_shortcut
  40. contextlib2
  41. cryptography
  42. curl
  43. cycler
  44. cython
  45. cytoolz
  46. dask
  47. dask-core
  48. datashape
  49. decorator
  50. defusedxml
  51. distributed
  52. docutils
  53. entrypoints
  54. et_xmlfile
  55. fast-histogram
  56. fastcache
  57. filelock
  58. flask
  59. flask-cors
  60. freetype
  61. future
  62. get_terminal_size
  63. gevent
  64. glob2
  65. greenlet
  66. h5py
  67. hdf5
  68. heapdict
  69. html5lib
  70. icc_rt
  71. icu
  72. idna
  73. imageio
  74. imagesize
  75. importlib_metadata
  76. intel-openmp
  77. ipykernel
  78. ipython
  79. ipython_genutils
  80. ipywidgets
  81. isort
  82. itsdangerous
  83. jdcal
  84. jedi
  85. jinja2
  86. jpeg
  87. jsonschema
  88. jupyter
  89. jupyter_client
  90. jupyter_console
  91. jupyter_core
  92. jupyterlab
  93. jupyter_server
  94. keyring
  95. kiwisolver
  96. krb5
  97. lazy-object-proxy
  98. libarchive
  99. libcurl
  100. libiconv
  101. libpng
  102. libprotobuf
  103. libsodium
  104. libssh2
  105. libtiff
  106. libxml2
  107. libxslt
  108. llvmlite
  109. locket
  110. lxml
  111. lz4-c
  112. lzo
  113. m2w64-gcc-libgfortran
  114. m2w64-gcc-libs
  115. m2w64-gcc-libs-core
  116. m2w64-gmp
  117. m2w64-libwinpthread-git
  118. markupsafe
  119. <span style="color:red">***matplotlib***</span>
  120. <span style="color:red">***matplotlib-tests***</span>
  121. mccabe
  122. menuinst
  123. mistune
  124. mkl
  125. mkl-service
  126. mkl_fft
  127. mkl_random
  128. more-itertools
  129. mpl-scatter-density
  130. mpmath
  131. msgpack-numpy
  132. msgpack-python
  133. mysys2-conda-epoch
  134. multipledispatch
  135. <span style="color:red">***mysql-connector-c***</span>
  136. <span style="color:red">***mysql-connector-python***</span>
  137. <span style="color:red">***mysqlclient***</span>
  138. navigator-updater
  139. nbconvert
  140. nbformat
  141. networkx
  142. nltk
  143. nose
  144. notebook
  145. numba
  146. numexpr
  147. <span style="color:red">***numpy***</span>
  148. <span style="color:red">***numpy-base***</span>
  149. <span style="color:red">***numpy-devel***</span>
  150. <span style="color:red">***numpydoc***</span>
  151. odo
  152. olefile
  153. openpyxl
  154. openssl
  155. packaging
  156. <span style="color:red">***pandas***</span>
  157. pandoc
  158. pandocfilters
  159. parso
  160. partd
  161. path.py
  162. pathlib2
  163. patsy
  164. pep8
  165. pickleshare
  166. pillow
  167. pip
  168. pkginfo
  169. pluggy
  170. ply
  171. prometheus_client
  172. prompt_toolkit
  173. protobuf
  174. psutil
  175. py
  176. pycodestyle
  177. pycosat
  178. pycparser
  179. pycrypto
  180. pycurl
  181. pyflakes
  182. pygments
  183. pylint
  184. pymysql
  185. pyodbc
  186. pyopenssl
  187. pyparsing
  188. pyqt
  189. pysocks
  190. pytables
  191. pytest
  192. pytest-arraydiff
  193. pytest-astropy
  194. pytest-doctestplus
  195. pytest-openfiles
  196. pytest-remotedata
  197. python
  198. python-dateutil
  199. python-libarchive-c
  200. pytz
  201. pywavelets
  202. pywin32
  203. pywinpty
  204. pyyaml
  205. pyzmq
  206. qt
  207. qtawesome
  208. qtconsole
  209. qtpy
  210. requests
  211. rope
  212. ruamel_yaml
  213. <span style="color:red">***scikit-image***</span>
  214. <span style="color:red">***scikit-learn***</span>
  215. <span style="color:red">***scipy***</span>
  216. <span style="color:red">***seaborn***</span>
  217. send2trash
  218. setuptools
  219. simplegeneric
  220. singledispatch
  221. sip
  222. six
  223. snappy
  224. snowballstemmer
  225. sortedcollections
  226. sortedcontainers
  227. sphinx
  228. sphinxcontrib
  229. sphinxcontrib-websupport
  230. spyder
  231. spyder-kernels
  232. sqlalchemy
  233. sqlite
  234. <span style="color:red">***statsmodels***</span>
  235. sympy
  236. tblib
  237. terminado
  238. testpath
  239. tk
  240. toolz
  241. tornado
  242. tqdm
  243. traitlets
  244. unicodecsv
  245. urllib3
  246. vc
  247. vs2015_runtime
  248. wcwidth
  249. webencodings
  250. werkzeug
  251. wheel
  252. widgetsnbextension
  253. wein_inet_pton
  254. win_unicode_con
  255. wincerststore
  256. winpty
  257. wrapt
  258. xlrd
  259. xlsxwriter
  260. xlwings
  261. xlwt
  262. xz
  263. yaml
  264. zeromq
  265. zict
  266. zlib
  267. zstd
   
# MYSQL Setup
* When installing MySQL, you can choose to setup specific details for admin options - I set it to default so that login credentials are simple (root/password).
* The schema for the database is called odendata - MySQL doesn't allow caps for names. The tables are setup as follows:
* Key: AI = auto increment, PK = primary key
   | Table Name | Field(s) (type/details) | Use/Reason for existing |
   | ---------- | ----------------------- | ----------------------- |
   | massdata   | uuid (text), metric(text), timestamp (text), value(double) | Imported data from odenDump |
   | criticalmetrics | id(int AI PK), metricname(varchar 45) | Metrics that were discovered to predict a failure in the line. |
   | emails | id(int AI PK), email(varchar 60) | Details of who to email in case of a predicted failure. |
   | forecasteddata |  predictedValue(float), predictedDate(varchar 45), predictedLowerValue (float), predictedUpperValue (float), metricName (varchar 45) | The data that gets pushed to the database from the forecasted model. This data should be read from the power bi display through queries. |
   | motorloadmetering | value (double), timestamp (text) | The clean data that is transferred from massdata for the motor load metering variable. |
   | motorrpmpullout | value (double), timestamp (text) | The clean data that is transferred from massdata for the motor rpm pullout variable. |
* The values in the critical metrics table must be an actual metric value in the massdata table. When transferring values from the mass data table to the table that corresponds to the metric name - ensure that the values are the same. i.e. If I grab the metric name 'test_var' from odendata.massdata, it is imperative that the table created is called 'test_var', and the entry added in odendata.criticalmetrics is 'test_var'. By doing this, it ensures that the program does not face errors when making a query for data that does not exist.
* Suppose the table 'test_var' does not exist, yet the odendata.criticalmetrics table contains that value to track, the program will encounter an error as it will not know what to access (since it does not exist).

# Python
### LocalLibrary:
* Library uses the OS and shell utilities.
* ROOT refers to the source file's working folder.
* _selectBaseData grabs all values from the massdata table.
* _selectColumnHeaders is a common call to grab the values and timestamps from the specific metric tables.
* Functions:
  1. OutputToFileNameQuery(fileName): outputs information to the /dump/ folder based off of the root defined earlier and creates a .csv file with the filename specified via the parameter given. This function is used for the modeling and forecasting.
  2. DeleteCSVFile(f): deletes the /dump/ files if they exist - if the data is constantly updated, then it would make sense to delete the old CSV files. Duplicates are not intended.
  3. SelectMetric(metricName): selects all records from the specific metric name, ordered by the timestamp value.
  4. GetMetrics(): returns the list of metric variables to watch.
  5. InsertForecast(): inserts dynamic data to be inserted into the forecasted data table.
  6. ClearForecast(): Truncates (drops/deletes all rows) the table 'forecastdata'.
  7. DeleteImageFolder(path): clears all images in the folder that were created. The reasoning behind this method is that there should be new images generated any time a new model gets created, and thus there should be new images that correspond to it.

### DataTransmitter:
* Imports locallibrary and the mysql connector.
* _VARIABLES refer to the global values for the database connection or the email information containers.
* Note that when setting up the MySQL server, this information is critical to have match with the credentials set up during set-up. Complete access is required for the script, thus privileges need to be set in order for this to work.
* Functions
  1. Class DatabaseClient:
     1. init: creates a connection to the server given the details, and sets up the cursor.
     2. setQuery(self, newQ): sets query to newQ.
     3. getQuery(self): returns the query that is currently set.
     4. performQuery(self): executes the current query on the database.
     5. getCursor(self): returns the cursor.
     6. getPreparsedCursor(self): returns the cursor that is prepared for multiple statement executions at once.
     7. close(self): closes the cursor and connection.
     8. comm(self): commits multiple SQL executions, used in tandem with getPreparedCursor(self).
  2. SendEmails(): sends informations regarding the current report to the recipients stored in the database table 'emails'. uses MIME libraries to allow multiple images and recipients and message parts. Reads images from the /images/ folder.
  3. addEmailText(strToAdd): adds string strToAdd to the email body before being sent.
  4. addEmailImage(imgName): adds imgName to the image attachments for the email to be sent.

### DataModeler:
* All of the magic happens here!
* When running the program for the first time on boot, it takes a while to perform the imports on all of the libraries. The first ~20 lines are all imports and are required for the program to work.
* The program executes in the following manner:
  * Set up database client and get the metrics we are following
  * loop through metric variables and do the following:
    * delete any existing CSV files that were used for prior models
    * sets a query to grab the relevant information for the current metric (from specific metric table) - exports this data to a CSV
    * adds the query to the email if needed to be sent
    * read the data with pandas into a trainSet
    * calls on the functions DataPlot, SARIMAXPlot, DataAnalysis, and UploadForecast
    * deletes the images created
    * go to next metric
  * clears the forecast table in the database
  * closes the database connection
* Functions:
  1. DataPlot(dataSet): This function sets the plot and sampling of the data taken in through dataSet. It strips the ' UTC' portion of the string from the timestamp (as UTC is not part of a proper timestamp), and sets it to be an index. Next it plots the data as a line over time. It returns the sampled data on a business day rate by the values column.
  2. SARIMAXPlot(dataSet, iterator, predictionDate): This function trains a model and creates a predicted mean plot with some statistical graphics. Initially, it takes in the dataSet in the SARIMAX function and the results get fitted. Next it saves an image of the graph it created and then shows it. It saves the images based on the name, which is iterator. Afterwards, it gets a prediction with a confidence reading and plots it as a function of time. The prediction is made from the predictionDate and onwards. This is the one step ahead forecast, which is saved and added in the email. The return values are the predicted means, and the results.
  3. DataAnalysis(dataSet, results, iterator, stepsAhead, startDate): This function creates a forecast for stepsAhead steps past the startDate. Documented is the MSE and RMSE, which is attached to the email. Next we create a graph and predict the next stepsAhead steps. We save this image and attach it to the email as well. Afterwords we reccord the predicted mean and standard deviation. The lower the deviation the better. After all of this is done, we determine if an email should be sent. Currently, no statistical analysis is done for this portion, only a broad 'guess'. A for loop iterates through the predicted means (in the forecasted values), and for each value, it compares itself to the predicted mean - .5 standard deviations. If one of the rows meets this value (which is approximately a 17% dip from the average), we send out an email for the timestamp it was found in. The return values are the predicted mean and the conf_int of the pred_uc.
  4. Upload Forecast(predData1, predData2, metric, steps): This function adds the forecast data to the table forecasteddata in the database. it loops through all of the data in predData1 and 2 and adds it to an array. Next, it loops through the entire array, and adds each row as a new record to the table.